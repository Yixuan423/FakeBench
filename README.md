# [TIFS2025]FakeBench: Probing Explainable Fake Image Detection via Large Multimodal Models

:octocat:The released data and evaluation codes for the paper entitled "**FakeBench: Probing Explainable Fake Image Detection via Large Multimodal Models**"

**FakeBench** [<img src="https://public.boxcloud.com/api/2.0/files/774931772678/content?preview=true&version=832360847987&access_token=1!L51xTr_tAZkHYIZfzIlCq58bh-T_80kC-ktqRYOPeZgJR8DGCIlc_yz1c0YzYAwjNzfY24F9TAJDlEVs941I7Q_PwWKlrKaDCXx9f04MgjvgWr20O36woXX6P6RA7ACpuayC8bI8XrSUZDVbeHIrVmXen-4yBGd9o0Ny7NGI69EldvzwbhgwDgBfhLlJh30106nxRqNsnwfsr6g_f2SS0jH__YuVMjpCmRxp3mBkr7yoMSu67_fo7_iVbQHYWj6U7u9klwg1tfHzDq9ZdBPAWybHPpfRsAjqgQyFlR5MIMcRVkfTmpsIvd9CPOsrf0R62f3DTWwYyOdx1lVrlNC3oe3dH5xDe0TFmA4ZfQ2CMHcFgdVmcA4xF4f3G6-B7iNYBJKVv1umX0R7I3o8xB8dpqfEd0rN3lWnNntfnIlM2mDrKbEZWl8e-f3QWlb7-LkHKnW3SF96U4KnxrMXP2aYx60BaYT7Cgvfx4n90bYhUHAszV-cZhjsihBM1ecFoPxc5-1F1tButdl_gBA1L9S5xxcrCJmPvQ8VmuAmK2J_PQDUs5CzcRtVb4QLgmm7eE_djtyAxIpsKPop3Fx8ra00PYoxHBQmyCp_mTy0ng..&shared_link=https%3A%2F%2Fcornell.app.box.com%2Fv%2Farxiv-logo-svg&box_client_name=box-content-preview&box_client_version=2.110.0" width="40" height="40">](https://arxiv.org/abs/2404.13306)


:speak_no_evil:Please be notified that we **do not** release the labeled data to avoid corpus leakage but only **evaluation queries and codes**.

:ok_woman:You are highly welcome to use the evaluation codes and submit your model responses to us to obtain the performance measures.

:ok_woman:Go to the **Submission Guideline** below for more details.

:bow:If you find our work useful, please give us a star :star2: for this repository！

## Brief Introduction
<img src = "https://github.com/Yixuan423/FakeBench/blob/master/figures/intro.png" width ="400px">

The ability to distinguish whether an image is generated by artificial intelligence (AI) is a crucial ingredient in human intelligence, usually accompanied by a complex and dialectical forensic and reasoning process. However, current fake image detection models and databases focus on binary classification without understandable explanations for the general populace. This weakens the credibility of authenticity judgment and may conceal potential model biases. Meanwhile, large multimodal models (LMMs) have exhibited immense vision-language capabilities on various tasks, bringing the potential for explainable fake image detection. Therefore, we pioneered the probe of LMMs for explainable fake image detection by presenting a multimodal database encompassing descriptions of textual authenticity, the FakeBench. For construction, we first introduce a fine-grained taxonomy of generative visual forgery concerning human perception, based on which we collect forgery descriptions in human natural language with a human-in-the-loop strategy. FakeBench examines LMMs with four evaluation criteria: __detection__, __reasoning__, __interpretation__, and __fine-grained forgery analysis__, to obtain deeper insights into image authenticity-relevant capabilities. Experiments on various LMMs confirm their merits and demerits in different aspects of fake image detection tasks. This research presents a paradigm shift towards __transparency__ for the fake image detection area and reveals the need for greater emphasis on forensic elements in visual-language research and AI risk control.

## Image Data
The 6,000 fake and real images can be downloaded via:[https://portland-my.sharepoint.com/:f:/g/personal/yixli5-c_my_cityu_edu_hk/EoGF50mSDkxNoRzzQq8xZHAB8sVd6Ab2NN57W5nDaChEVQ?e=1fIWUw](https://drive.google.com/file/d/1RaL4oMURQKuF6oHI505y243TiIUJ6aD3/view?usp=sharing)

## Submission Guidelines
To avoid corpus leakage, the labeled data is not publicly released, and only the query data can be publicly obtained. You are encouraged to submit your model's response to us following the steps below:
### Step 1. Download the images.
Please download the image part of the FakeBench and place the images under the FakeBench_images folder. This folder contains two subfolders by default, including `fake_images` and `real_images`. You need to manually switch the real and fake image folders to test them respectively, or you can put all images together under one folder and change the testing dir correspondingly.

`NOTE: Please note that FakeBench is a purely scientific research, non-profit, non-commercial project; the use of the process of strict compliance with Creative Commons Attribution-NonCommercial (CC BY-NC), such as the use of the data, is found to be training models; we reserve the right to take ALL measures.`
### Step 2. Evaluation
Replace your ChatModel in each `eval_*.py`. The example is based on the GeminiPro v1.

（1）Evaluate on FakeClass:

`python eval/eval_FakeClass.py`

（2）Evaluate on FakeClue:

`python eval/eval_FakeClue.py`

`Note that FakeClue contains two evaluation modes, i.e., faultfinding mode and inference mode. If you don't need all the two modes, comment the related codes in the .py file.`

(3) Evaluate on FakeQA:

`python eval/eval_FakeQA.py`

### Step 3: Submit the results

**Note:** Please make sure that all your results are in the **UTF-8 FORMAT**.
You should get 4 `.json` files after evaluation. Please submit them to us together with the name of your tested MLLM through one of the following:
* Yixuan Li, `yixuanli423@gmail.com`
* Xuelin Liu, `xuelinliu-bill@foxmail.com`

We will return the evaluation feedback to you in no time.

## The Leaderboard
:smiley:**Any models wanna be tested on FakeBench, please contact us!**
### Detection
<img src = "https://github.com/Yixuan423/FakeBench/blob/master/figures/result1.png" width ="700px">

### Interpretation
<img src = "https://github.com/Yixuan423/FakeBench/blob/master/figures/result2.png" width ="500px">

### Reasoning
<img src = "https://github.com/Yixuan423/FakeBench/blob/master/figures/result3.png" width ="500px">

### Finge-grained Analysis
<img src = "https://github.com/Yixuan423/FakeBench/blob/master/figures/result4.png" width ="500px">

## Citation Information
If you find our paper useful, please kindly cite it.
```
@article{li2024fakebench,
  title={FakeBench: Uncover the Achilles' Heels of Fake Images with Large Multimodal Models},
  author={Li, Yixuan and Liu, Xuelin and Wang, Xiaoyang and Wang, Shiqi and Lin, Weisi},
  journal={arXiv preprint arXiv:2404.13306},
  year={2024}
}

```

